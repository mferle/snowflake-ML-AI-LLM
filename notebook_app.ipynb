{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4dcc0a2-dd4a-4290-8064-53e5fd3a5acc",
   "metadata": {
    "name": "Intro",
    "collapsed": false,
    "resultHeight": 512
   },
   "source": "# Snowflake built-in ML functions\n\nThe Snowflake ML functions enable you to extract predictions and insights from your data using machine learning. You don’t have to be a machine learning expert to use them.\n\n## Time-Series Functions\nTrain machine learning models on time-series data to determine how a specified metric (for example, sales) varies over time. The model provides insights or predictions based on the trends detected in the data.\n\n- **Forecasting:** predict future values from past trends\n- **Anomaly Detection:** flag values that deviate from expected values\n\n## Other Analysis Functions\nThese models don’t require time series data, but benefit from a large number of features.\n\n- **Classification:** map rows into two or more classes based on their most predictive features\n- **Top Insights:** helps you find dimensions and values that affect the metric\n"
  },
  {
   "cell_type": "markdown",
   "id": "26110c24-6cbb-4567-8c2b-4397d1c3c0ef",
   "metadata": {
    "name": "Intro_credit_scoring",
    "collapsed": false,
    "resultHeight": 337
   },
   "source": "# Credit scoring model\nCredit scoring models evaluate an applicant's features like their demographic information, payment history, number of accounts, credit types, and other financial information to calculate a credit score. \n\nThe credit score is used by lending institutions to determine how risky it is to lend money to the applicant.\n\nFor this demo, we will create a credit scoring model:\n- Get demo data from https://www.kaggle.com/datasets/parisrohan/credit-score-classification/ and copy it into the DEMO_DATA Snowflake table\n- Use the Snowflake Classification ML function\n\nThe account setup (role, database, schema, warehouse) and importing demo data are in the ```setup.sql``` script.\n"
  },
  {
   "cell_type": "code",
   "id": "dd10565b-4803-42af-8dc0-e7f0065ec1ad",
   "metadata": {
    "language": "sql",
    "name": "Demo_data",
    "collapsed": false,
    "resultHeight": 510
   },
   "outputs": [],
   "source": "select * from demo_data;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fe89b56f-aa78-4f57-b228-758c653f0795",
   "metadata": {
    "name": "Clean_demo_data",
    "collapsed": false,
    "resultHeight": 191
   },
   "source": "## Clean the demo data\n1. Remove underscores from numeric variables\n2. Convert the Month value into numeric so we can sort it\n3. Remove any records where the Age is less than 18 or greater than 100\n4. Calculate the credit history age into months from years and months"
  },
  {
   "cell_type": "code",
   "id": "732673d9-6af1-4904-b9b3-4b5f0abd2de8",
   "metadata": {
    "language": "sql",
    "name": "Clean_demo_data_sql",
    "resultHeight": 111,
    "collapsed": false
   },
   "outputs": [],
   "source": "-- clean demo data\ncreate or replace view demo_data_clean as\nselect \n  \"Customer_ID\" as CUST_ID, \n  month(to_date('2024-'||substr(\"Month\", 1, 3)||'-01', 'YYYY-Mon-DD')) as MTH,\n  case \n    when try_to_number(replace(\"Age\", '_', '')) < 18 then null\n    when try_to_number(replace(\"Age\", '_', '')) > 100 then null\n    else try_to_number(replace(\"Age\", '_', ''))\n  end as AGE,\n  case\n    when substr(\"Occupation\", 1, 2) = '__' then null\n    else \"Occupation\"\n  end as OCCUPATION,\n  to_number(replace(\"Annual_Income\", '_', '')) as ANNUAL_INCOME,\n  to_number(replace(\"Monthly_Inhand_Salary\", '_', '')) as MONTHLY_SALARY,\n  to_number(\"Delay_from_due_date\") as DELAY_FROM_DUE_DT,\n  \"Changed_Credit_Limit\" as CHANGED_CREDIT_LIMIT,\n  to_number(replace(\"Outstanding_Debt\", '_', '')) as OUTSTANDING_DEBT,\n  to_number(replace(\"Credit_Utilization_Ratio\", '_', '')) as CREDIT_UTIL_RATIO,\n  try_to_number(REGEXP_SUBSTR(\"Credit_History_Age\", '^([0-9]+)')) * 12 +\n    try_to_number(substr(REGEXP_SUBSTR(\"Credit_History_Age\", 'and ([0-9]+)'), 5, 2)) AS CREDIT_HIST_AGE_MTHS,\n  \"Payment_of_Min_Amount\" as PYMT_MIN_AMT,\n  to_number(replace(\"Amount_invested_monthly\", '_', '')) as AMT_INVESTED_MTHLY,\n  \"Payment_Behaviour\" as PYMT_BEH,\n  replace(\"Monthly_Balance\", '_', '') as MONTHLY_BALANCE,\n  \"Credit_Score\" as CREDIT_SCORE\nfrom demo_data;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8aa589ef-8a7b-4576-b1e5-90d64c35b8df",
   "metadata": {
    "name": "Aggregate_demo_data",
    "collapsed": false,
    "resultHeight": 174
   },
   "source": "## Aggregate the cleaned demo data\nThe demo data contains several months worth of data for each customer. We want to aggregate the data so that we have only one record per customer.\n1. Some variables are aggregated by ```last_value()```, eg. age, occupation, outstanding debt, etc.\n2. Some variables are aggregated by ```avg()```, eg. monthly salary, amount invested monthly, etc."
  },
  {
   "cell_type": "code",
   "id": "04e4f9a7-3664-436d-8113-f5863f1b98f1",
   "metadata": {
    "language": "sql",
    "name": "Aggregate_demo_data_sql",
    "collapsed": false,
    "resultHeight": 111
   },
   "outputs": [],
   "source": "\n-- aggregate demo data\ncreate or replace view demo_data_agg as\nwith aggregates as (\n  select cust_id, mth,\n    last_value(age) ignore nulls over (partition by cust_id order by mth desc) as age,\n    last_value(occupation) ignore nulls over (partition by cust_id order by mth desc) as occupation,\n    avg(annual_income) over (partition by cust_id) as avg_annual_income,\n    avg(monthly_salary) over (partition by cust_id) as avg_monthly_salary,\n    last_value(delay_from_due_dt) ignore nulls over (partition by cust_id order by mth desc) as delay_from_due_dt,\n    last_value(changed_credit_limit) ignore nulls over (partition by cust_id order by mth desc) as changed_credit_limit,\n    last_value(outstanding_debt) ignore nulls over (partition by cust_id order by mth desc) as outstanding_debt,\n    avg(credit_util_ratio) over (partition by cust_id) as credit_util_ratio,\n    last_value(credit_hist_age_mths) ignore nulls over (partition by cust_id order by mth desc) as credit_hist_age_mths,\n    last_value(pymt_min_amt) ignore nulls over (partition by cust_id order by mth desc) as pymt_min_amt,\n    avg(amt_invested_mthly) over (partition by cust_id) as amt_invested_mthly,\n    last_value(pymt_beh) ignore nulls over (partition by cust_id order by mth desc) as pymt_beh,\n    avg(monthly_balance) over (partition by cust_id) as monthly_balance,\n    last_value(credit_score) ignore nulls over (partition by cust_id order by mth desc) as credit_score\n  from demo_data_clean\n)\nselect * from aggregates where mth = 8;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1d0cd6a6-dc1c-47ad-9fc4-7ac8f9fa164f",
   "metadata": {
    "name": "cell2",
    "collapsed": false,
    "resultHeight": 170
   },
   "source": "## Exploratory data analysis\nBefore building any machine learning model, we perform exploratory data analysis to get familiar with the data, understand the distribution, identify any outliers, deal with missing values, etc.\n\nWe will look at the distribution of the values of the classification variable ```CREDIT_SCORE```."
  },
  {
   "cell_type": "code",
   "id": "5858005e-aabd-41aa-a5a6-2d5f2076b2d5",
   "metadata": {
    "language": "sql",
    "name": "View_agg_data",
    "collapsed": false,
    "resultHeight": 510
   },
   "outputs": [],
   "source": "select * from demo_data_agg;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "524ab9e3-7a9d-4c0c-a491-8b4f46fa7f95",
   "metadata": {
    "language": "sql",
    "name": "Credit_score_dist",
    "collapsed": false,
    "resultHeight": 181
   },
   "outputs": [],
   "source": "select credit_score, count(*) as cnt from demo_data_agg group by all;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "96888093-7670-4e9b-94b4-0917055b1ad1",
   "metadata": {
    "language": "python",
    "name": "Credit_score_dist_viz",
    "collapsed": false,
    "resultHeight": 372
   },
   "outputs": [],
   "source": "# Import python packages\nimport streamlit as st\n\ndf = Credit_score_dist.to_pandas()\ndf1 = df.set_index(df['CREDIT_SCORE'])\ndf1 = df1['CNT']\n\nst.bar_chart(df1)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "66bddbc4-e918-41ab-9e46-f45f56f0e84a",
   "metadata": {
    "name": "Train_test_split",
    "collapsed": false,
    "resultHeight": 164
   },
   "source": "## Split demo data into training and test:\n1. Create the training data by taking a 70% sample of the demo data\n2. Create the test data by taking the remaining 30% of the demo data\n3. Remove the ```cust_id``` and ```mth``` columns from the training data because they don't represent features for model training"
  },
  {
   "cell_type": "code",
   "id": "23aadd4c-c537-4a7a-8dfa-eb1887c8bcf6",
   "metadata": {
    "language": "sql",
    "name": "Train_test_split_sql",
    "collapsed": false,
    "resultHeight": 111
   },
   "outputs": [],
   "source": "-- create training data by taking 70% sample from the train_data_agg table\ncreate or replace table demo_data_samp70 as\nselect * \nfrom demo_data_agg\nSAMPLE (70);\n\n-- create test data by taking remaining 30% data\ncreate or replace table test_data as\nselect * \nfrom demo_data_agg \nwhere cust_id not in (select cust_id from demo_data_samp70);\n\n-- remove the cust_id and mth columns from the training data\ncreate or replace view train_data\nas select * exclude (cust_id, mth) from demo_data_samp70;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6aa92797-d15d-4c16-b245-5b56d47de839",
   "metadata": {
    "name": "Classification_ML_Function",
    "collapsed": false,
    "resultHeight": 103
   },
   "source": "## Use the Classification ML Function to create the credit scoring model\nUsing the ```TRAIN_DATA_SCORING``` data as input"
  },
  {
   "cell_type": "code",
   "id": "7415cbfb-c0f1-4cd0-b7d2-dcb47b4ff83b",
   "metadata": {
    "language": "sql",
    "name": "Classification_ML_Function_SQL",
    "collapsed": false,
    "resultHeight": 111
   },
   "outputs": [],
   "source": "create or replace SNOWFLAKE.ML.CLASSIFICATION credit_scoring_model (\n    input_data => TABLE(train_data),\n    target_colname => 'credit_score'\n);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5c5d590f-e6c8-4e96-bcb5-79e62a324f26",
   "metadata": {
    "name": "Predict",
    "collapsed": false,
    "resultHeight": 103
   },
   "source": "## Predict the credit score on the test data\nSave the predicted results to the ```MODEL_OUTPUT``` table. Parse the relevant columns, such as the predicted class and the probability from the returned JSON."
  },
  {
   "cell_type": "code",
   "id": "2fed3d8a-cb0b-48f9-b3e7-04f17fabdfc0",
   "metadata": {
    "language": "sql",
    "name": "Predict_SQL",
    "collapsed": false,
    "resultHeight": 111
   },
   "outputs": [],
   "source": "create or replace table model_output as\nselect *, \n  credit_scoring_model!PREDICT(\n    input_data => object_construct(*))\n    as prediction\nfrom test_data;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "af643954-8377-403e-a5f5-3874e765a2c6",
   "metadata": {
    "language": "sql",
    "name": "Parse_predict_SQL",
    "collapsed": false,
    "resultHeight": 438
   },
   "outputs": [],
   "source": "select *,\n  prediction:\"class\"::varchar as class,\n  prediction:\"probability\".\"Good\" as probability_good,\n  prediction:\"probability\".\"Standard\" as probability_standard,\n  prediction:\"probability\".\"Poor\" as probability_poor\nfrom model_output;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f433c96f-4b89-40be-9fde-d6ef11b6374a",
   "metadata": {
    "name": "Confusion_matrix",
    "collapsed": false,
    "resultHeight": 128
   },
   "source": "## Create the confusion matrix\nCompare the actual (column ```CREDIT_SCORE```) and the predicted (column ```CLASS```) and count the distinct pairs of predicted and actual, then plot the results as a heatmap."
  },
  {
   "cell_type": "code",
   "id": "9e3ac61d-b832-4851-a4d2-fcebceaeeff2",
   "metadata": {
    "language": "sql",
    "name": "Confusion_matrix_SQL",
    "collapsed": false,
    "resultHeight": 391
   },
   "outputs": [],
   "source": "select credit_score, prediction:\"class\"::varchar as class, count(cust_id) as cnt from model_output group by all;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f5411a3a-c574-4815-9579-709c537abbac",
   "metadata": {
    "language": "python",
    "name": "Confusion_matrix_heatmap",
    "collapsed": false,
    "resultHeight": 646
   },
   "outputs": [],
   "source": "import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = Confusion_matrix_SQL.to_pandas()\nheatmap_data = df.pivot_table(index='CREDIT_SCORE', columns='CLASS', values='CNT', aggfunc='sum', fill_value=0)\n\nfig, ax = plt.subplots(figsize=(4, 3))\nsns.heatmap(heatmap_data, annot=True, cmap='YlGnBu', fmt='d', ax=ax)\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b4656c72-c54e-487a-94e3-6de1e20c1d44",
   "metadata": {
    "name": "Evaluate_model",
    "collapsed": false,
    "resultHeight": 103
   },
   "source": "## Evaluate model performance\nCall the ```SHOW_EVALUATION_METRICS()``` and ```SHOW_FEATURE_IMPORTANCE()``` functions and evaluate the results"
  },
  {
   "cell_type": "code",
   "id": "6ab30b04-3b28-4c20-9e27-3115956b2542",
   "metadata": {
    "language": "sql",
    "name": "Show_evaluation_metrics",
    "collapsed": false,
    "resultHeight": 438
   },
   "outputs": [],
   "source": "call credit_scoring_model!SHOW_EVALUATION_METRICS();",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "74127d52-9c78-46e1-93a5-f68d37c4f700",
   "metadata": {
    "language": "sql",
    "name": "Show_feature_importance",
    "collapsed": false,
    "resultHeight": 438
   },
   "outputs": [],
   "source": "call credit_scoring_model!SHOW_FEATURE_IMPORTANCE();",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3e181dc1-31ba-4b8d-9f14-eab013b7d1a2",
   "metadata": {
    "name": "Balanced_classes",
    "collapsed": false,
    "resultHeight": 163
   },
   "source": "## Create another model using training data with better balanced classes\n- take 100% of the Good class\n- take 30% of the Standard class\n- take 55% of the Poor class"
  },
  {
   "cell_type": "code",
   "id": "9e4dd587-3663-45bd-8c62-33808e55413c",
   "metadata": {
    "language": "sql",
    "name": "Balanced_classes_SQL",
    "collapsed": false,
    "resultHeight": 181
   },
   "outputs": [],
   "source": "-- take sample data for better balanced classes\ncreate or replace table train_data_bal as\nselect * from train_data where credit_score = 'Good' \nunion all\nselect * from (select * from train_data where credit_score = 'Standard') SAMPLE (30) \nunion all\nselect * from (select * from train_data where credit_score = 'Poor') SAMPLE (55);\n\nselect credit_score, count(*) as cnt from train_data_bal group by all;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f83fbcf1-4dc0-4899-8144-fbae078251bb",
   "metadata": {
    "name": "Train_and_predict",
    "collapsed": false,
    "resultHeight": 102
   },
   "source": "## Train the model and predict\nUse the same test data to predict as previously"
  },
  {
   "cell_type": "code",
   "id": "1111361d-18c6-4c03-8123-6fb586e8f9e8",
   "metadata": {
    "language": "sql",
    "name": "Train_and_predict_SQL",
    "collapsed": false,
    "resultHeight": 111
   },
   "outputs": [],
   "source": "create or replace SNOWFLAKE.ML.CLASSIFICATION credit_scoring_model_bal (\n    input_data => TABLE(train_data_bal),\n    target_colname => 'credit_score'\n);\n\ncreate or replace table model_output_bal as\nselect *, \n  credit_scoring_model_bal!PREDICT(\n    input_data => object_construct(*))\n    as prediction\nfrom test_data;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "13d9dfa0-e6dd-4dee-a10a-1bb14e0812fa",
   "metadata": {
    "name": "Evaluate_new_model",
    "collapsed": false,
    "resultHeight": 102
   },
   "source": "## Evaluate the new model\nPlot the confusion matrix"
  },
  {
   "cell_type": "code",
   "id": "437f667a-0346-4d19-9151-f8cff2dc2646",
   "metadata": {
    "language": "sql",
    "name": "Confusion_matrix_SQL_bal",
    "collapsed": false,
    "resultHeight": 391
   },
   "outputs": [],
   "source": "select credit_score, prediction:\"class\"::varchar as class, count(cust_id) as cnt from model_output_bal group by all;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eeba7209-2694-4810-920e-678daf745251",
   "metadata": {
    "language": "python",
    "name": "Confusion_matrix_heatmap_bal",
    "collapsed": false,
    "resultHeight": 649
   },
   "outputs": [],
   "source": "import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = Confusion_matrix_SQL_bal.to_pandas()\nheatmap_data = df.pivot_table(index='CREDIT_SCORE', columns='CLASS', values='CNT', aggfunc='sum', fill_value=0)\n\nfig, ax = plt.subplots(figsize=(4, 3))\nsns.heatmap(heatmap_data, annot=True, cmap='YlGnBu', fmt='d', ax=ax)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "447452dd-ff4a-4818-b853-ac3398fffd83",
   "metadata": {
    "language": "sql",
    "name": "Evaluation_metrics_bal",
    "collapsed": false,
    "resultHeight": 438
   },
   "outputs": [],
   "source": "call credit_scoring_model_bal!SHOW_EVALUATION_METRICS();",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ef4fc291-7479-4eb1-a2d3-85a8dc33ae5a",
   "metadata": {
    "name": "cell1",
    "collapsed": false,
    "resultHeight": 249
   },
   "source": "## The importance of exploratory data analysis\n- find the distribution of feature values to identify underrepresented groups\n- identify outliers and remove them if they distort the distribution\n- deal with missing values (remove, impute, average, etc.)\n- calculated aggregated values (features)\n- balance the classes\n- take data samples that represent the feature groups more evenly"
  },
  {
   "cell_type": "markdown",
   "id": "7a6f1f49-fb0d-40f2-adab-814e7ece62c1",
   "metadata": {
    "name": "cell3",
    "collapsed": false,
    "resultHeight": 198
   },
   "source": "## Next steps\nExplore some of the newer functionality related to machine learning, such as:\n- Register the model in the model registry for use by the organization (https://docs.snowflake.com/en/developer-guide/snowflake-ml/model-registry/overview)\n- Use the feature store to calculate aggregated or derived values on a regular basis (https://docs.snowflake.com/en/developer-guide/snowflake-ml/feature-store/overview)"
  }
 ]
}